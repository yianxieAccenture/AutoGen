{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /Users/yian.xie/Library/Python/3.9/lib/python/site-packages (23.3)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyautogen\n",
      "  Downloading pyautogen-0.1.12-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.2/75.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting flaml\n",
      "  Using cached FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Collecting NumPy>=1.17.0rc1\n",
      "  Downloading numpy-1.26.1-cp310-cp310-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-macosx_10_9_x86_64.whl (368 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m369.0/369.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.20\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.0-cp310-cp310-macosx_10_9_x86_64.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp310-cp310-macosx_10_9_x86_64.whl (65 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-macosx_10_9_x86_64.whl (46 kB)\n",
      "Installing collected packages: urllib3, tqdm, termcolor, python-dotenv, NumPy, multidict, idna, frozenlist, diskcache, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, flaml, aiosignal, aiohttp, openai, pyautogen\n",
      "Successfully installed NumPy-1.26.1 aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.0 diskcache-5.6.3 flaml-2.1.1 frozenlist-1.4.0 idna-3.4 multidict-6.0.4 openai-0.28.1 pyautogen-0.1.12 python-dotenv-1.0.0 requests-2.31.0 termcolor-2.3.0 tqdm-4.66.1 urllib3-2.0.7 yarl-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ! /Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\n",
    "# ! pip install pyautogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @classmethod\n",
      "    def _get_response(cls, config: Dict, raise_on_ratelimit_or_timeout=False, use_cache=True):\n",
      "        \"\"\"Get the response from the openai api call.\n",
      "\n",
      "        Try cache first. If not found, call the openai api. If the api call fails, retry after retry_wait_time.\n",
      "        \"\"\"\n",
      "        config = config.copy()\n",
      "        openai.api_key_path = config.pop(\"api_key_path\", openai.api_key_path)\n",
      "        key = get_key(config)\n",
      "        if use_cache:\n",
      "            response = cls._cache.get(key, None)\n",
      "            if response is not None and (response != -1 or not raise_on_ratelimit_or_timeout):\n",
      "                # print(\"using cached response\")\n",
      "                cls._book_keeping(config, response)\n",
      "                return response\n",
      "        openai_completion = (\n",
      "            openai.ChatCompletion\n",
      "            if config[\"model\"].replace(\"gpt-35-turbo\", \"gpt-3.5-turbo\") in cls.chat_models\n",
      "            or issubclass(cls, ChatCompletion)\n",
      "            else openai.Completion\n",
      "        )\n",
      "        start_time = time.time()\n",
      "        request_timeout = cls.request_timeout\n",
      "        max_retry_period = config.pop(\"max_retry_period\", cls.max_retry_period)\n",
      "        retry_wait_time = config.pop(\"retry_wait_time\", cls.retry_wait_time)\n",
      "        while True:\n",
      "            try:\n",
      "                if \"request_timeout\" in config:\n",
      "                    response = openai_completion.create(**config)\n",
      "                else:\n",
      "                    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "            except (\n",
      "                ServiceUnavailableError,\n",
      "                APIConnectionError,\n",
      "            ):\n",
      "                # transient error\n",
      "                logger.info(f\"retrying in {retry_wait_time} seconds...\", exc_info=1)\n",
      "                sleep(retry_wait_time)\n",
      "            except APIError as err:\n",
      "                error_code = err and err.json_body and isinstance(err.json_body, dict) and err.json_body.get(\"error\")\n",
      "                error_code = error_code and error_code.get(\"code\")\n",
      "                if error_code == \"content_filter\":\n",
      "                    raise\n",
      "                # transient error\n",
      "                logger.info(f\"retrying in {retry_wait_time} seconds...\", exc_info=1)\n",
      "                sleep(retry_wait_time)\n",
      "            except (RateLimitError, Timeout) as err:\n",
      "                time_left = max_retry_period - (time.time() - start_time + retry_wait_time)\n",
      "                if (\n",
      "                    time_left > 0\n",
      "                    and isinstance(err, RateLimitError)\n",
      "                    or time_left > request_timeout\n",
      "                    and isinstance(err, Timeout)\n",
      "                    and \"request_timeout\" not in config\n",
      "                ):\n",
      "                    if isinstance(err, Timeout):\n",
      "                        request_timeout <<= 1\n",
      "                    request_timeout = min(request_timeout, time_left)\n",
      "                    logger.info(f\"retrying in {retry_wait_time} seconds...\", exc_info=1)\n",
      "                    sleep(retry_wait_time)\n",
      "                elif raise_on_ratelimit_or_timeout:\n",
      "                    raise\n",
      "                else:\n",
      "                    response = -1\n",
      "                    if use_cache and isinstance(err, Timeout):\n",
      "                        cls._cache.set(key, response)\n",
      "                    logger.warning(\n",
      "                        f\"Failed to get response from openai api due to getting RateLimitError or Timeout for {max_retry_period} seconds.\"\n",
      "                    )\n",
      "                    return response\n",
      "            except InvalidRequestError:\n",
      "                if \"azure\" in config.get(\"api_type\", openai.api_type) and \"model\" in config:\n",
      "                    # azure api uses \"engine\" instead of \"model\"\n",
      "                    config[\"engine\"] = config.pop(\"model\").replace(\"gpt-3.5-turbo\", \"gpt-35-turbo\")\n",
      "                else:\n",
      "                    raise\n",
      "            else:\n",
      "                if use_cache:\n",
      "                    cls._cache.set(key, response)\n",
      "                cls._book_keeping(config, response)\n",
      "                return response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the function\n",
    "\n",
    "import autogen\n",
    "import inspect\n",
    "\n",
    "\n",
    "completion_instance = autogen.Completion()\n",
    "# Assuming 'create' is a method of the 'Completion' class\n",
    "create_method = completion_instance._get_response\n",
    "print(inspect.getsource(create_method))\n",
    "# Get information about the function using inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to Assistant):\n",
      "\n",
      "Plot a chart of NVDA and TESLA stock price change YTD.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 10-20 11:43:26] {234} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py\", line 220, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Internal server error {\n",
      "    \"error\": {\n",
      "        \"message\": \"Internal server error\",\n",
      "        \"type\": \"auth_subrequest_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"internal_error\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Internal server error', 'type': 'auth_subrequest_error', 'param': None, 'code': 'internal_error'}} {'Date': 'Fri, 20 Oct 2023 01:13:28 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '166', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': 'b10bef05dadc4d3bdd0fcf14fc498bef', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '818d65e33cdc1f5e-MEL', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 10-20 11:43:36] {234} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py\", line 220, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 20 Oct 2023 01:13:39 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '818d66428ba31f5e-MEL', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py:220\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39;49mcreate(request_timeout\u001b[39m=\u001b[39;49mrequest_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[1;32m    222\u001b[0m     ServiceUnavailableError,\n\u001b[1;32m    223\u001b[0m     APIConnectionError,\n\u001b[1;32m    224\u001b[0m ):\n\u001b[1;32m    225\u001b[0m     \u001b[39m# transient error\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m (\n\u001b[1;32m    141\u001b[0m     deployment_id,\n\u001b[1;32m    142\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m     url,\n\u001b[1;32m    158\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m     request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m     request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m     method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m )\n\u001b[0;32m--> 299\u001b[0m resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAPIError\u001b[0m: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 20 Oct 2023 01:13:39 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '818d66428ba31f5e-MEL', 'alt-svc': 'h3=\":443\"; ma=86400'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# assistant = newAssistantAgent(\"newAssistant\", llm_config={\"config_list\": config_list})\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m user_proxy \u001b[39m=\u001b[39m UserProxyAgent(\u001b[39m\"\u001b[39m\u001b[39muser_proxy\u001b[39m\u001b[39m\"\u001b[39m, code_execution_config\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mwork_dir\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcoding\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(assistant, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPlot a chart of NVDA and TESLA stock price change YTD.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py:799\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    797\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    798\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 799\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    800\u001b[0m         context,\n\u001b[1;32m    801\u001b[0m         use_cache,\n\u001b[1;32m    802\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    803\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    804\u001b[0m     )\n\u001b[1;32m    805\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    806\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py:830\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    829\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 830\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py:235\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39m# transient error\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mretrying in \u001b[39m\u001b[39m{\u001b[39;00mretry_wait_time\u001b[39m}\u001b[39;00m\u001b[39m seconds...\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m     sleep(retry_wait_time)\n\u001b[1;32m    236\u001b[0m \u001b[39mexcept\u001b[39;00m (RateLimitError, Timeout) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    237\u001b[0m     time_left \u001b[39m=\u001b[39m max_retry_period \u001b[39m-\u001b[39m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m+\u001b[39m retry_wait_time)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use to check autogen working fine\n",
    "from autogen import AssistantAgent , UserProxyAgent, config_list_from_json\n",
    "\n",
    "\n",
    "\n",
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "assistant = AssistantAgent(\"Assistant\", llm_config={\"config_list\": config_list})\n",
    "# assistant = newAssistantAgent(\"newAssistant\", llm_config={\"config_list\": config_list})\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\"})\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Set, Union\n",
    "import logging\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "\n",
    "NON_CACHE_KEY = [\"api_key\", \"api_base\", \"api_type\", \"api_version\"]\n",
    "\n",
    "\n",
    "def get_key(config):\n",
    "    \"\"\"Get a unique identifier of a configuration.\n",
    "\n",
    "    Args:\n",
    "        config (dict or list): A configuration.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A unique identifier which can be used as a key for a dict.\n",
    "    \"\"\"\n",
    "    copied = False\n",
    "    for key in NON_CACHE_KEY:\n",
    "        if key in config:\n",
    "            config, copied = config.copy() if not copied else config, True\n",
    "            config.pop(key)\n",
    "    # if isinstance(config, dict):\n",
    "    #     return tuple(get_key(x) for x in sorted(config.items()))\n",
    "    # if isinstance(config, list):\n",
    "    #     return tuple(get_key(x) for x in config)\n",
    "    # return config\n",
    "    return json.dumps(config, sort_keys=True)\n",
    "\n",
    "\n",
    "def get_config_list(\n",
    "    api_keys: List, api_bases: Optional[List] = None, api_type: Optional[str] = None, api_version: Optional[str] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Get a list of configs for openai api calls.\n",
    "\n",
    "    Args:\n",
    "        api_keys (list): The api keys for openai api calls.\n",
    "        api_bases (list, optional): The api bases for openai api calls.\n",
    "        api_type (str, optional): The api type for openai api calls.\n",
    "        api_version (str, optional): The api version for openai api calls.\n",
    "    \"\"\"\n",
    "    config_list = []\n",
    "    for i, api_key in enumerate(api_keys):\n",
    "        if not api_key.strip():\n",
    "            continue\n",
    "        config = {\"api_key\": api_key}\n",
    "        if api_bases:\n",
    "            config[\"api_base\"] = api_bases[i]\n",
    "        if api_type:\n",
    "            config[\"api_type\"] = api_type\n",
    "        if api_version:\n",
    "            config[\"api_version\"] = api_version\n",
    "        config_list.append(config)\n",
    "    return config_list\n",
    "\n",
    "\n",
    "def config_list_openai_aoai(\n",
    "    key_file_path: Optional[str] = \".\",\n",
    "    openai_api_key_file: Optional[str] = \"key_openai.txt\",\n",
    "    aoai_api_key_file: Optional[str] = \"key_aoai.txt\",\n",
    "    aoai_api_base_file: Optional[str] = \"base_aoai.txt\",\n",
    "    exclude: Optional[str] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Get a list of configs for openai + azure openai api calls.\n",
    "\n",
    "    Args:\n",
    "        key_file_path (str, optional): The path to the key files.\n",
    "        openai_api_key_file (str, optional): The file name of the openai api key.\n",
    "        aoai_api_key_file (str, optional): The file name of the azure openai api key.\n",
    "        aoai_api_base_file (str, optional): The file name of the azure openai api base.\n",
    "        exclude (str, optional): The api type to exclude, \"openai\" or \"aoai\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of configs for openai api calls.\n",
    "    \"\"\"\n",
    "    if \"OPENAI_API_KEY\" not in os.environ and exclude != \"openai\":\n",
    "        try:\n",
    "            with open(f\"{key_file_path}/{openai_api_key_file}\") as key_file:\n",
    "                os.environ[\"OPENAI_API_KEY\"] = key_file.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            logging.info(\n",
    "                \"To use OpenAI API, please set OPENAI_API_KEY in os.environ \"\n",
    "                \"or create key_openai.txt in the specified path, or specify the api_key in config_list.\"\n",
    "            )\n",
    "    if \"AZURE_OPENAI_API_KEY\" not in os.environ and exclude != \"aoai\":\n",
    "        try:\n",
    "            with open(f\"{key_file_path}/{aoai_api_key_file}\") as key_file:\n",
    "                os.environ[\"AZURE_OPENAI_API_KEY\"] = key_file.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            logging.info(\n",
    "                \"To use Azure OpenAI API, please set AZURE_OPENAI_API_KEY in os.environ \"\n",
    "                \"or create key_aoai.txt in the specified path, or specify the api_key in config_list.\"\n",
    "            )\n",
    "    if \"AZURE_OPENAI_API_BASE\" not in os.environ and exclude != \"aoai\":\n",
    "        try:\n",
    "            with open(f\"{key_file_path}/{aoai_api_base_file}\") as key_file:\n",
    "                os.environ[\"AZURE_OPENAI_API_BASE\"] = key_file.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            logging.info(\n",
    "                \"To use Azure OpenAI API, please set AZURE_OPENAI_API_BASE in os.environ \"\n",
    "                \"or create base_aoai.txt in the specified path, or specify the api_base in config_list.\"\n",
    "            )\n",
    "    aoai_config = (\n",
    "        get_config_list(\n",
    "            # Assuming Azure OpenAI api keys in os.environ[\"AZURE_OPENAI_API_KEY\"], in separated lines\n",
    "            api_keys=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\").split(\"\\n\"),\n",
    "            # Assuming Azure OpenAI api bases in os.environ[\"AZURE_OPENAI_API_BASE\"], in separated lines\n",
    "            api_bases=os.environ.get(\"AZURE_OPENAI_API_BASE\", \"\").split(\"\\n\"),\n",
    "            api_type=\"azure\",\n",
    "            api_version=\"2023-07-01-preview\",  # change if necessary\n",
    "        )\n",
    "        if exclude != \"aoai\"\n",
    "        else []\n",
    "    )\n",
    "    openai_config = (\n",
    "        get_config_list(\n",
    "            # Assuming OpenAI API_KEY in os.environ[\"OPENAI_API_KEY\"]\n",
    "            api_keys=os.environ.get(\"OPENAI_API_KEY\", \"\").split(\"\\n\"),\n",
    "            # \"api_type\": \"open_ai\",\n",
    "            # \"api_base\": \"https://api.openai.com/v1\",\n",
    "        )\n",
    "        if exclude != \"openai\"\n",
    "        else []\n",
    "    )\n",
    "    config_list = openai_config + aoai_config\n",
    "    return config_list\n",
    "\n",
    "\n",
    "def config_list_from_models(\n",
    "    key_file_path: Optional[str] = \".\",\n",
    "    openai_api_key_file: Optional[str] = \"key_openai.txt\",\n",
    "    aoai_api_key_file: Optional[str] = \"key_aoai.txt\",\n",
    "    aoai_api_base_file: Optional[str] = \"base_aoai.txt\",\n",
    "    exclude: Optional[str] = None,\n",
    "    model_list: Optional[list] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Get a list of configs for api calls with models in the model list.\n",
    "\n",
    "    Args:\n",
    "        key_file_path (str, optional): The path to the key files.\n",
    "        openai_api_key_file (str, optional): The file name of the openai api key.\n",
    "        aoai_api_key_file (str, optional): The file name of the azure openai api key.\n",
    "        aoai_api_base_file (str, optional): The file name of the azure openai api base.\n",
    "        exclude (str, optional): The api type to exclude, \"openai\" or \"aoai\".\n",
    "        model_list (list, optional): The model list.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of configs for openai api calls.\n",
    "    \"\"\"\n",
    "    config_list = config_list_openai_aoai(\n",
    "        key_file_path,\n",
    "        openai_api_key_file,\n",
    "        aoai_api_key_file,\n",
    "        aoai_api_base_file,\n",
    "        exclude,\n",
    "    )\n",
    "    if model_list:\n",
    "        config_list = [{**config, \"model\": model} for model in model_list for config in config_list]\n",
    "    return config_list\n",
    "\n",
    "\n",
    "def config_list_gpt4_gpt35(\n",
    "    key_file_path: Optional[str] = \".\",\n",
    "    openai_api_key_file: Optional[str] = \"key_openai.txt\",\n",
    "    aoai_api_key_file: Optional[str] = \"key_aoai.txt\",\n",
    "    aoai_api_base_file: Optional[str] = \"base_aoai.txt\",\n",
    "    exclude: Optional[str] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Get a list of configs for gpt-4 followed by gpt-3.5 api calls.\n",
    "\n",
    "    Args:\n",
    "        key_file_path (str, optional): The path to the key files.\n",
    "        openai_api_key_file (str, optional): The file name of the openai api key.\n",
    "        aoai_api_key_file (str, optional): The file name of the azure openai api key.\n",
    "        aoai_api_base_file (str, optional): The file name of the azure openai api base.\n",
    "        exclude (str, optional): The api type to exclude, \"openai\" or \"aoai\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of configs for openai api calls.\n",
    "    \"\"\"\n",
    "    return config_list_from_models(\n",
    "        key_file_path,\n",
    "        openai_api_key_file,\n",
    "        aoai_api_key_file,\n",
    "        aoai_api_base_file,\n",
    "        exclude,\n",
    "        model_list=[\"gpt-4\", \"gpt-3.5-turbo\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_config(config_list, filter_dict):\n",
    "    \"\"\"Filter the config list by provider and model.\n",
    "\n",
    "    Args:\n",
    "        config_list (list): The config list.\n",
    "        filter_dict (dict, optional): The filter dict with keys corresponding to a field in each config,\n",
    "            and values corresponding to lists of acceptable values for each key.\n",
    "\n",
    "    Returns:\n",
    "        list: The filtered config list.\n",
    "    \"\"\"\n",
    "    if filter_dict:\n",
    "        config_list = [\n",
    "            config for config in config_list if all(config.get(key) in value for key, value in filter_dict.items())\n",
    "        ]\n",
    "    return config_list\n",
    "\n",
    "\n",
    "def config_list_from_json(\n",
    "    env_or_file: str,\n",
    "    file_location: Optional[str] = \"\",\n",
    "    filter_dict: Optional[Dict[str, Union[List[Union[str, None]], Set[Union[str, None]]]]] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Get a list of configs from a json parsed from an env variable or a file.\n",
    "\n",
    "    Args:\n",
    "        env_or_file (str): The env variable name or file name.\n",
    "        file_location (str, optional): The file location.\n",
    "        filter_dict (dict, optional): The filter dict with keys corresponding to a field in each config,\n",
    "            and values corresponding to lists of acceptable values for each key.\n",
    "            e.g.,\n",
    "    ```python\n",
    "    filter_dict = {\n",
    "        \"api_type\": [\"open_ai\", None],  # None means a missing key is acceptable\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Returns:\n",
    "        list: A list of configs for openai api calls.\n",
    "    \"\"\"\n",
    "    json_str = os.environ.get(env_or_file)\n",
    "    if json_str:\n",
    "        config_list = json.loads(json_str)\n",
    "    else:\n",
    "        config_list_path = os.path.join(file_location, env_or_file)\n",
    "        try:\n",
    "            with open(config_list_path) as json_file:\n",
    "                config_list = json.load(json_file)\n",
    "        except FileNotFoundError:\n",
    "            logging.warning(f\"The specified config_list file '{config_list_path}' does not exist.\")\n",
    "            return []\n",
    "    return filter_config(config_list, filter_dict)\n",
    "\n",
    "\n",
    "def get_config(\n",
    "    api_key: str, api_base: Optional[str] = None, api_type: Optional[str] = None, api_version: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Construct a configuration dictionary with the provided API configurations.\n",
    "    Appending the additional configurations to the config only if they're set\n",
    "\n",
    "    example:\n",
    "    >> model_api_key_map={\n",
    "        \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        \"gpt-3.5-turbo\": {\n",
    "            \"api_key_env_var\": \"ANOTHER_API_KEY\",\n",
    "            \"api_type\": \"aoai\",\n",
    "            \"api_version\": \"v2\",\n",
    "            \"api_base\": \"https://api.someotherapi.com\"\n",
    "        }\n",
    "    }\n",
    "    Args:\n",
    "        api_key (str): The API key used for authenticating API requests.\n",
    "        api_base (str, optional): The base URL of the API. Defaults to None.\n",
    "        api_type (str, optional): The type or kind of API. Defaults to None.\n",
    "        api_version (str, optional): The API version. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A dictionary containing the API configurations.\n",
    "    \"\"\"\n",
    "    config = {\"api_key\": api_key}\n",
    "    if api_base:\n",
    "        config[\"api_base\"] = api_base\n",
    "    if api_type:\n",
    "        config[\"api_type\"] = api_type\n",
    "    if api_version:\n",
    "        config[\"api_version\"] = api_version\n",
    "    return config\n",
    "\n",
    "\n",
    "def config_list_from_dotenv(\n",
    "    dotenv_file_path: Optional[str] = None, model_api_key_map: Optional[dict] = None, filter_dict: Optional[dict] = None\n",
    ") -> List[Dict[str, Union[str, Set[str]]]]:\n",
    "    \"\"\"\n",
    "    Load API configurations from a specified .env file or environment variables and construct a list of configurations.\n",
    "\n",
    "    This function will:\n",
    "    - Load API keys from a provided .env file or from existing environment variables.\n",
    "    - Create a configuration dictionary for each model using the API keys and additional configurations.\n",
    "    - Filter and return the configurations based on provided filters.\n",
    "\n",
    "    model_api_key_map will default to `{\"gpt-4\": \"OPENAI_API_KEY\", \"gpt-3.5-turbo\": \"OPENAI_API_KEY\"}` if none\n",
    "\n",
    "    Args:\n",
    "        dotenv_file_path (str, optional): The path to the .env file. Defaults to None.\n",
    "        model_api_key_map (str/dict, optional): A dictionary mapping models to their API key configurations.\n",
    "                                           If a string is provided as configuration, it is considered as an environment\n",
    "                                           variable name storing the API key.\n",
    "                                           If a dict is provided, it should contain at least 'api_key_env_var' key,\n",
    "                                           and optionally other API configurations like 'api_base', 'api_type', and 'api_version'.\n",
    "                                           Defaults to a basic map with 'gpt-4' and 'gpt-3.5-turbo' mapped to 'OPENAI_API_KEY'.\n",
    "        filter_dict (dict, optional): A dictionary containing the models to be loaded.\n",
    "                                      Containing a 'model' key mapped to a set of model names to be loaded.\n",
    "                                      Defaults to None, which loads all found configurations.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Union[str, Set[str]]]]: A list of configuration dictionaries for each model.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified .env file does not exist.\n",
    "        TypeError: If an unsupported type of configuration is provided in model_api_key_map.\n",
    "    \"\"\"\n",
    "    if dotenv_file_path:\n",
    "        dotenv_path = Path(dotenv_file_path)\n",
    "        if dotenv_path.exists():\n",
    "            load_dotenv(dotenv_path)\n",
    "        else:\n",
    "            logging.warning(f\"The specified .env file {dotenv_path} does not exist.\")\n",
    "    else:\n",
    "        dotenv_path = find_dotenv()\n",
    "        if not dotenv_path:\n",
    "            logging.warning(\"No .env file found. Loading configurations from environment variables.\")\n",
    "        load_dotenv(dotenv_path)\n",
    "\n",
    "    # Ensure the model_api_key_map is not None to prevent TypeErrors during key assignment.\n",
    "    model_api_key_map = model_api_key_map or {}\n",
    "\n",
    "    # Ensure default models are always considered\n",
    "    default_models = [\"gpt-4\", \"gpt-3.5-turbo\"]\n",
    "\n",
    "    for model in default_models:\n",
    "        # Only assign default API key if the model is not present in the map.\n",
    "        # If model is present but set to invalid/empty, do not overwrite.\n",
    "        if model not in model_api_key_map:\n",
    "            model_api_key_map[model] = \"OPENAI_API_KEY\"\n",
    "\n",
    "    env_var = []\n",
    "    # Loop over the models and create configuration dictionaries\n",
    "    for model, config in model_api_key_map.items():\n",
    "        if isinstance(config, str):\n",
    "            api_key_env_var = config\n",
    "            config_dict = get_config(api_key=os.getenv(api_key_env_var))\n",
    "        elif isinstance(config, dict):\n",
    "            api_key = os.getenv(config.get(\"api_key_env_var\", \"OPENAI_API_KEY\"))\n",
    "            config_without_key_var = {k: v for k, v in config.items() if k != \"api_key_env_var\"}\n",
    "            config_dict = get_config(api_key=api_key, **config_without_key_var)\n",
    "        else:\n",
    "            logging.warning(f\"Unsupported type {type(config)} for model {model} configuration\")\n",
    "\n",
    "        if not config_dict[\"api_key\"] or config_dict[\"api_key\"].strip() == \"\":\n",
    "            logging.warning(\n",
    "                f\"API key not found or empty for model {model}. Please ensure path to .env file is correct.\"\n",
    "            )\n",
    "            continue  # Skip this configuration and continue with the next\n",
    "\n",
    "        # Add model to the configuration and append to the list\n",
    "        config_dict[\"model\"] = model\n",
    "        env_var.append(config_dict)\n",
    "\n",
    "    fd, temp_name = tempfile.mkstemp()\n",
    "    try:\n",
    "        with os.fdopen(fd, \"w+\") as temp:\n",
    "            env_var_str = json.dumps(env_var)\n",
    "            temp.write(env_var_str)\n",
    "            temp.flush()\n",
    "\n",
    "            # Assuming config_list_from_json is a valid function from your code\n",
    "            config_list = config_list_from_json(env_or_file=temp_name, filter_dict=filter_dict)\n",
    "    finally:\n",
    "        # The file is deleted after using its name (to prevent windows build from breaking)\n",
    "        os.remove(temp_name)\n",
    "\n",
    "    if len(config_list) == 0:\n",
    "        logging.error(\"No configurations loaded.\")\n",
    "        return []\n",
    "\n",
    "    logging.info(f\"Models available: {[config['model'] for config in config_list]}\")\n",
    "    return config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Optional, Dict, Callable, Union\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "from flaml import tune, BlendSearch\n",
    "from flaml.tune.space import is_constant\n",
    "from flaml.automl.logger import logger_formatter\n",
    "# from .openai_utils import get_key\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    from openai.error import (\n",
    "        ServiceUnavailableError,\n",
    "        RateLimitError,\n",
    "        APIError,\n",
    "        InvalidRequestError,\n",
    "        APIConnectionError,\n",
    "        Timeout,\n",
    "        AuthenticationError,\n",
    "    )\n",
    "    from openai import Completion as openai_Completion\n",
    "    import diskcache\n",
    "\n",
    "    ERROR = None\n",
    "except ImportError:\n",
    "    ERROR = ImportError(\"please install openai and diskcache to use the autogen.oai subpackage.\")\n",
    "    openai_Completion = object\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.handlers:\n",
    "    # Add the console handler.\n",
    "    _ch = logging.StreamHandler(stream=sys.stdout)\n",
    "    _ch.setFormatter(logger_formatter)\n",
    "    logger.addHandler(_ch)\n",
    "\n",
    "\n",
    "class Completion(openai_Completion):\n",
    "    \"\"\"A class for OpenAI completion API.\n",
    "\n",
    "    It also supports: ChatCompletion, Azure OpenAI API.\n",
    "    \"\"\"\n",
    "\n",
    "    # set of models that support chat completion\n",
    "    chat_models = {\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-3.5-turbo-0301\",  # deprecate in Sep\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-35-turbo\",\n",
    "        \"gpt-4\",\n",
    "        \"gpt-4-32k\",\n",
    "        \"gpt-4-32k-0314\",  # deprecate in Sep\n",
    "        \"gpt-4-0314\",  # deprecate in Sep\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "    }\n",
    "\n",
    "    # price per 1k tokens\n",
    "    price1K = {\n",
    "        \"text-ada-001\": 0.0004,\n",
    "        \"text-babbage-001\": 0.0005,\n",
    "        \"text-curie-001\": 0.002,\n",
    "        \"code-cushman-001\": 0.024,\n",
    "        \"code-davinci-002\": 0.1,\n",
    "        \"text-davinci-002\": 0.02,\n",
    "        \"text-davinci-003\": 0.02,\n",
    "        \"gpt-3.5-turbo\": (0.0015, 0.002),\n",
    "        \"gpt-3.5-turbo-0301\": (0.0015, 0.002),  # deprecate in Sep\n",
    "        \"gpt-3.5-turbo-0613\": (0.0015, 0.002),\n",
    "        \"gpt-3.5-turbo-16k\": (0.003, 0.004),\n",
    "        \"gpt-3.5-turbo-16k-0613\": (0.003, 0.004),\n",
    "        \"gpt-35-turbo\": 0.002,\n",
    "        \"gpt-4\": (0.03, 0.06),\n",
    "        \"gpt-4-32k\": (0.06, 0.12),\n",
    "        \"gpt-4-0314\": (0.03, 0.06),  # deprecate in Sep\n",
    "        \"gpt-4-32k-0314\": (0.06, 0.12),  # deprecate in Sep\n",
    "        \"gpt-4-0613\": (0.03, 0.06),\n",
    "        \"gpt-4-32k-0613\": (0.06, 0.12),\n",
    "    }\n",
    "\n",
    "    default_search_space = {\n",
    "        \"model\": tune.choice(\n",
    "            [\n",
    "                \"text-ada-001\",\n",
    "                \"text-babbage-001\",\n",
    "                \"text-davinci-003\",\n",
    "                \"gpt-3.5-turbo\",\n",
    "                \"gpt-4\",\n",
    "            ]\n",
    "        ),\n",
    "        \"temperature_or_top_p\": tune.choice(\n",
    "            [\n",
    "                {\"temperature\": tune.uniform(0, 2)},\n",
    "                {\"top_p\": tune.uniform(0, 1)},\n",
    "            ]\n",
    "        ),\n",
    "        \"max_tokens\": tune.lograndint(50, 1000),\n",
    "        \"n\": tune.randint(1, 100),\n",
    "        \"prompt\": \"{prompt}\",\n",
    "    }\n",
    "\n",
    "    seed = 41\n",
    "    cache_path = f\".cache/{seed}\"\n",
    "    # retry after this many seconds\n",
    "    retry_wait_time = 10\n",
    "    # fail a request after hitting RateLimitError for this many seconds\n",
    "    max_retry_period = 120\n",
    "    # time out for request to openai server\n",
    "    request_timeout = 60\n",
    "\n",
    "    openai_completion_class = not ERROR and openai.Completion\n",
    "    _total_cost = 0\n",
    "    optimization_budget = None\n",
    "\n",
    "    _history_dict = _count_create = None\n",
    "\n",
    "    @classmethod\n",
    "    def set_cache(cls, seed: Optional[int] = 41, cache_path_root: Optional[str] = \".cache\"):\n",
    "        \"\"\"Set cache path.\n",
    "\n",
    "        Args:\n",
    "            seed (int, Optional): The integer identifier for the pseudo seed.\n",
    "                Results corresponding to different seeds will be cached in different places.\n",
    "            cache_path (str, Optional): The root path for the cache.\n",
    "                The complete cache path will be {cache_path}/{seed}.\n",
    "        \"\"\"\n",
    "        cls.seed = seed\n",
    "        cls.cache_path = f\"{cache_path_root}/{seed}\"\n",
    "\n",
    "    @classmethod\n",
    "    def clear_cache(cls, seed: Optional[int] = None, cache_path_root: Optional[str] = \".cache\"):\n",
    "        \"\"\"Clear cache.\n",
    "\n",
    "        Args:\n",
    "            seed (int, Optional): The integer identifier for the pseudo seed.\n",
    "                If omitted, all caches under cache_path_root will be cleared.\n",
    "            cache_path (str, Optional): The root path for the cache.\n",
    "                The complete cache path will be {cache_path}/{seed}.\n",
    "        \"\"\"\n",
    "        if seed is None:\n",
    "            shutil.rmtree(cache_path_root, ignore_errors=True)\n",
    "            return\n",
    "        with diskcache.Cache(f\"{cache_path_root}/{seed}\") as cache:\n",
    "            cache.clear()\n",
    "\n",
    "    @classmethod\n",
    "    def _book_keeping(cls, config: Dict, response):\n",
    "        \"\"\"Book keeping for the created completions.\"\"\"\n",
    "        if response != -1 and \"cost\" not in response:\n",
    "            response[\"cost\"] = cls.cost(response)\n",
    "        if cls._history_dict is None:\n",
    "            return\n",
    "        if cls._history_compact:\n",
    "            value = {\n",
    "                \"created_at\": [],\n",
    "                \"cost\": [],\n",
    "            }\n",
    "            if \"messages\" in config:\n",
    "                messages = config[\"messages\"]\n",
    "                if len(messages) > 1 and messages[-1][\"role\"] != \"assistant\":\n",
    "                    existing_key = get_key(messages[:-1])\n",
    "                    value = cls._history_dict.pop(existing_key, value)\n",
    "                key = get_key(messages + [choice[\"message\"] for choice in response[\"choices\"]])\n",
    "            else:\n",
    "                key = get_key([config[\"prompt\"]] + [choice.get(\"text\") for choice in response[\"choices\"]])\n",
    "            value[\"created_at\"].append(cls._count_create)\n",
    "            value[\"cost\"].append(response[\"cost\"])\n",
    "            cls._history_dict[key] = value\n",
    "            cls._count_create += 1\n",
    "            return\n",
    "        cls._history_dict[cls._count_create] = {\n",
    "            \"request\": config,\n",
    "            \"response\": response.to_dict_recursive(),\n",
    "        }\n",
    "        cls._count_create += 1\n",
    "\n",
    "    @classmethod\n",
    "    def _get_response(cls, config: Dict, raise_on_ratelimit_or_timeout=False, use_cache=True):\n",
    "        \"\"\"Get the response from the openai api call.\n",
    "\n",
    "        Try cache first. If not found, call the openai api. If the api call fails, retry after retry_wait_time.\n",
    "        \"\"\"\n",
    "        config = config.copy()\n",
    "        openai.api_key_path = config.pop(\"api_key_path\", openai.api_key_path)\n",
    "        key = get_key(config)\n",
    "        if use_cache:\n",
    "            response = cls._cache.get(key, None)\n",
    "            if response is not None and (response != -1 or not raise_on_ratelimit_or_timeout):\n",
    "                # print(\"using cached response\")\n",
    "                cls._book_keeping(config, response)\n",
    "                return response\n",
    "        openai_completion = (\n",
    "            openai.ChatCompletion\n",
    "            if config[\"model\"].replace(\"gpt-35-turbo\", \"gpt-3.5-turbo\") in cls.chat_models\n",
    "            or issubclass(cls, ChatCompletion)\n",
    "            else openai.Completion\n",
    "        )\n",
    "        start_time = time.time()\n",
    "        request_timeout = cls.request_timeout\n",
    "        max_retry_period = config.pop(\"max_retry_period\", cls.max_retry_period)\n",
    "        retry_wait_time = config.pop(\"retry_wait_time\", cls.retry_wait_time)\n",
    "        while True:\n",
    "            try:\n",
    "                if \"request_timeout\" in config:\n",
    "                    response = openai_completion.create(**config)\n",
    "                else:\n",
    "                    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
    "            except (\n",
    "                ServiceUnavailableError,\n",
    "                APIConnectionError,\n",
    "            ):\n",
    "                # transient error\n",
    "                logger.info(f\"retrying in {retry_wait_time} seconds...\", exc_info=1)\n",
    "                sleep(retry_wait_time)\n",
    "            except APIError as err:\n",
    "                error_code = err and err.json_body and isinstance(err.json_body, dict) and err.json_body.get(\"error\")\n",
    "                error_code = error_code and error_code.get(\"code\")\n",
    "                if error_code == \"content_filter\":\n",
    "                    raise\n",
    "                # transient error\n",
    "                logger.info(f\"retrying in {retry_wait_time} seconds...\", exc_info=1)\n",
    "                sleep(retry_wait_time)\n",
    "            except (RateLimitError, Timeout) as err:\n",
    "                time_left = max_retry_period - (time.time() - start_time + retry_wait_time)\n",
    "                if (\n",
    "                    time_left > 0\n",
    "                    and isinstance(err, RateLimitError)\n",
    "                    or time_left > request_timeout\n",
    "                    and isinstance(err, Timeout)\n",
    "                    and \"request_timeout\" not in config\n",
    "                ):\n",
    "                    if isinstance(err, Timeout):\n",
    "                        request_timeout <<= 1\n",
    "                    request_timeout = min(request_timeout, time_left)\n",
    "                    logger.info(f\"retrying in {retry_wait_time} seconds...\", exc_info=1)\n",
    "                    sleep(retry_wait_time)\n",
    "                elif raise_on_ratelimit_or_timeout:\n",
    "                    raise\n",
    "                else:\n",
    "                    response = -1\n",
    "                    if use_cache and isinstance(err, Timeout):\n",
    "                        cls._cache.set(key, response)\n",
    "                    logger.warning(\n",
    "                        f\"Failed to get response from openai api due to getting RateLimitError or Timeout for {max_retry_period} seconds.\"\n",
    "                    )\n",
    "                    return response\n",
    "            except InvalidRequestError:\n",
    "                if \"azure\" in config.get(\"api_type\", openai.api_type) and \"model\" in config:\n",
    "                    # azure api uses \"engine\" instead of \"model\"\n",
    "                    config[\"engine\"] = config.pop(\"model\").replace(\"gpt-3.5-turbo\", \"gpt-35-turbo\")\n",
    "                else:\n",
    "                    raise\n",
    "            else:\n",
    "                if use_cache:\n",
    "                    cls._cache.set(key, response)\n",
    "                cls._book_keeping(config, response)\n",
    "                return response\n",
    "\n",
    "    @classmethod\n",
    "    def _get_max_valid_n(cls, key, max_tokens):\n",
    "        # find the max value in max_valid_n_per_max_tokens\n",
    "        # whose key is equal or larger than max_tokens\n",
    "        return max(\n",
    "            (value for k, value in cls._max_valid_n_per_max_tokens.get(key, {}).items() if k >= max_tokens),\n",
    "            default=1,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_min_invalid_n(cls, key, max_tokens):\n",
    "        # find the min value in min_invalid_n_per_max_tokens\n",
    "        # whose key is equal or smaller than max_tokens\n",
    "        return min(\n",
    "            (value for k, value in cls._min_invalid_n_per_max_tokens.get(key, {}).items() if k <= max_tokens),\n",
    "            default=None,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_region_key(cls, config):\n",
    "        # get a key for the valid/invalid region corresponding to the given config\n",
    "        config = cls._pop_subspace(config, always_copy=False)\n",
    "        return (\n",
    "            config[\"model\"],\n",
    "            config.get(\"prompt\", config.get(\"messages\")),\n",
    "            config.get(\"stop\"),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _update_invalid_n(cls, prune, region_key, max_tokens, num_completions):\n",
    "        if prune:\n",
    "            # update invalid n and prune this config\n",
    "            cls._min_invalid_n_per_max_tokens[region_key] = invalid_n = cls._min_invalid_n_per_max_tokens.get(\n",
    "                region_key, {}\n",
    "            )\n",
    "            invalid_n[max_tokens] = min(num_completions, invalid_n.get(max_tokens, np.inf))\n",
    "\n",
    "    @classmethod\n",
    "    def _pop_subspace(cls, config, always_copy=True):\n",
    "        if \"subspace\" in config:\n",
    "            config = config.copy()\n",
    "            config.update(config.pop(\"subspace\"))\n",
    "        return config.copy() if always_copy else config\n",
    "\n",
    "    @classmethod\n",
    "    def _get_params_for_create(cls, config: Dict) -> Dict:\n",
    "        \"\"\"Get the params for the openai api call from a config in the search space.\"\"\"\n",
    "        params = cls._pop_subspace(config)\n",
    "        if cls._prompts:\n",
    "            params[\"prompt\"] = cls._prompts[config[\"prompt\"]]\n",
    "        else:\n",
    "            params[\"messages\"] = cls._messages[config[\"messages\"]]\n",
    "        if \"stop\" in params:\n",
    "            params[\"stop\"] = cls._stops and cls._stops[params[\"stop\"]]\n",
    "        temperature_or_top_p = params.pop(\"temperature_or_top_p\", None)\n",
    "        if temperature_or_top_p:\n",
    "            params.update(temperature_or_top_p)\n",
    "        if cls._config_list and \"config_list\" not in params:\n",
    "            params[\"config_list\"] = cls._config_list\n",
    "        return params\n",
    "\n",
    "    @classmethod\n",
    "    def _eval(cls, config: dict, prune=True, eval_only=False):\n",
    "        \"\"\"Evaluate the given config as the hyperparameter setting for the openai api call.\n",
    "\n",
    "        Args:\n",
    "            config (dict): Hyperparameter setting for the openai api call.\n",
    "            prune (bool, optional): Whether to enable pruning. Defaults to True.\n",
    "            eval_only (bool, optional): Whether to evaluate only\n",
    "              (ignore the inference budget and do not rasie error when a request fails).\n",
    "              Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict: Evaluation results.\n",
    "        \"\"\"\n",
    "        cost = 0\n",
    "        data = cls.data\n",
    "        params = cls._get_params_for_create(config)\n",
    "        model = params[\"model\"]\n",
    "        data_length = len(data)\n",
    "        price = cls.price1K.get(model)\n",
    "        price_input, price_output = price if isinstance(price, tuple) else (price, price)\n",
    "        inference_budget = getattr(cls, \"inference_budget\", None)\n",
    "        prune_hp = getattr(cls, \"_prune_hp\", \"n\")\n",
    "        metric = cls._metric\n",
    "        config_n = params.get(prune_hp, 1)  # default value in OpenAI is 1\n",
    "        max_tokens = params.get(\n",
    "            \"max_tokens\", np.inf if model in cls.chat_models or issubclass(cls, ChatCompletion) else 16\n",
    "        )\n",
    "        target_output_tokens = None\n",
    "        if not cls.avg_input_tokens:\n",
    "            input_tokens = [None] * data_length\n",
    "        prune = prune and inference_budget and not eval_only\n",
    "        if prune:\n",
    "            region_key = cls._get_region_key(config)\n",
    "            max_valid_n = cls._get_max_valid_n(region_key, max_tokens)\n",
    "            if cls.avg_input_tokens:\n",
    "                target_output_tokens = (inference_budget * 1000 - cls.avg_input_tokens * price_input) / price_output\n",
    "                # max_tokens bounds the maximum tokens\n",
    "                # so using it we can calculate a valid n according to the avg # input tokens\n",
    "                max_valid_n = max(\n",
    "                    max_valid_n,\n",
    "                    int(target_output_tokens // max_tokens),\n",
    "                )\n",
    "            if config_n <= max_valid_n:\n",
    "                start_n = config_n\n",
    "            else:\n",
    "                min_invalid_n = cls._get_min_invalid_n(region_key, max_tokens)\n",
    "                if min_invalid_n is not None and config_n >= min_invalid_n:\n",
    "                    # prune this config\n",
    "                    return {\n",
    "                        \"inference_cost\": np.inf,\n",
    "                        metric: np.inf if cls._mode == \"min\" else -np.inf,\n",
    "                        \"cost\": cost,\n",
    "                    }\n",
    "                start_n = max_valid_n + 1\n",
    "        else:\n",
    "            start_n = config_n\n",
    "            region_key = None\n",
    "        num_completions, previous_num_completions = start_n, 0\n",
    "        n_tokens_list, result, responses_list = [], {}, []\n",
    "        while True:  # n <= config_n\n",
    "            params[prune_hp] = num_completions - previous_num_completions\n",
    "            data_limit = 1 if prune else data_length\n",
    "            prev_data_limit = 0\n",
    "            data_early_stop = False  # whether data early stop happens for this n\n",
    "            while True:  # data_limit <= data_length\n",
    "                # limit the number of data points to avoid rate limit\n",
    "                for i in range(prev_data_limit, data_limit):\n",
    "                    logger.debug(f\"num_completions={num_completions}, data instance={i}\")\n",
    "                    data_i = data[i]\n",
    "                    response = cls.create(data_i, raise_on_ratelimit_or_timeout=eval_only, **params)\n",
    "                    if response == -1:  # rate limit/timeout error, treat as invalid\n",
    "                        cls._update_invalid_n(prune, region_key, max_tokens, num_completions)\n",
    "                        result[metric] = 0\n",
    "                        result[\"cost\"] = cost\n",
    "                        return result\n",
    "                    # evaluate the quality of the responses\n",
    "                    responses = cls.extract_text_or_function_call(response)\n",
    "                    usage = response[\"usage\"]\n",
    "                    n_input_tokens = usage[\"prompt_tokens\"]\n",
    "                    n_output_tokens = usage.get(\"completion_tokens\", 0)\n",
    "                    if not cls.avg_input_tokens and not input_tokens[i]:\n",
    "                        # store the # input tokens\n",
    "                        input_tokens[i] = n_input_tokens\n",
    "                    query_cost = response[\"cost\"]\n",
    "                    cls._total_cost += query_cost\n",
    "                    cost += query_cost\n",
    "                    if cls.optimization_budget and cls._total_cost >= cls.optimization_budget and not eval_only:\n",
    "                        # limit the total tuning cost\n",
    "                        return {\n",
    "                            metric: 0,\n",
    "                            \"total_cost\": cls._total_cost,\n",
    "                            \"cost\": cost,\n",
    "                        }\n",
    "                    if previous_num_completions:\n",
    "                        n_tokens_list[i] += n_output_tokens\n",
    "                        responses_list[i].extend(responses)\n",
    "                        # Assumption 1: assuming requesting n1, n2 responses separatively then combining them\n",
    "                        # is the same as requesting (n1+n2) responses together\n",
    "                    else:\n",
    "                        n_tokens_list.append(n_output_tokens)\n",
    "                        responses_list.append(responses)\n",
    "                avg_n_tokens = np.mean(n_tokens_list[:data_limit])\n",
    "                rho = (\n",
    "                    (1 - data_limit / data_length) * (1 + 1 / data_limit)\n",
    "                    if data_limit << 1 > data_length\n",
    "                    else (1 - (data_limit - 1) / data_length)\n",
    "                )\n",
    "                # Hoeffding-Serfling bound\n",
    "                ratio = 0.1 * np.sqrt(rho / data_limit)\n",
    "                if target_output_tokens and avg_n_tokens > target_output_tokens * (1 + ratio) and not eval_only:\n",
    "                    cls._update_invalid_n(prune, region_key, max_tokens, num_completions)\n",
    "                    result[metric] = 0\n",
    "                    result[\"total_cost\"] = cls._total_cost\n",
    "                    result[\"cost\"] = cost\n",
    "                    return result\n",
    "                if (\n",
    "                    prune\n",
    "                    and target_output_tokens\n",
    "                    and avg_n_tokens <= target_output_tokens * (1 - ratio)\n",
    "                    and (num_completions < config_n or num_completions == config_n and data_limit == data_length)\n",
    "                ):\n",
    "                    # update valid n\n",
    "                    cls._max_valid_n_per_max_tokens[region_key] = valid_n = cls._max_valid_n_per_max_tokens.get(\n",
    "                        region_key, {}\n",
    "                    )\n",
    "                    valid_n[max_tokens] = max(num_completions, valid_n.get(max_tokens, 0))\n",
    "                    if num_completions < config_n:\n",
    "                        # valid already, skip the rest of the data\n",
    "                        data_limit = data_length\n",
    "                        data_early_stop = True\n",
    "                        break\n",
    "                prev_data_limit = data_limit\n",
    "                if data_limit < data_length:\n",
    "                    data_limit = min(data_limit << 1, data_length)\n",
    "                else:\n",
    "                    break\n",
    "            # use exponential search to increase n\n",
    "            if num_completions == config_n:\n",
    "                for i in range(data_limit):\n",
    "                    data_i = data[i]\n",
    "                    responses = responses_list[i]\n",
    "                    metrics = cls._eval_func(responses, **data_i)\n",
    "                    if result:\n",
    "                        for key, value in metrics.items():\n",
    "                            if isinstance(value, (float, int)):\n",
    "                                result[key] += value\n",
    "                    else:\n",
    "                        result = metrics\n",
    "                for key in result.keys():\n",
    "                    if isinstance(result[key], (float, int)):\n",
    "                        result[key] /= data_limit\n",
    "                result[\"total_cost\"] = cls._total_cost\n",
    "                result[\"cost\"] = cost\n",
    "                if not cls.avg_input_tokens:\n",
    "                    cls.avg_input_tokens = np.mean(input_tokens)\n",
    "                    if prune:\n",
    "                        target_output_tokens = (\n",
    "                            inference_budget * 1000 - cls.avg_input_tokens * price_input\n",
    "                        ) / price_output\n",
    "                result[\"inference_cost\"] = (avg_n_tokens * price_output + cls.avg_input_tokens * price_input) / 1000\n",
    "                break\n",
    "            else:\n",
    "                if data_early_stop:\n",
    "                    previous_num_completions = 0\n",
    "                    n_tokens_list.clear()\n",
    "                    responses_list.clear()\n",
    "                else:\n",
    "                    previous_num_completions = num_completions\n",
    "                num_completions = min(num_completions << 1, config_n)\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def tune(\n",
    "        cls,\n",
    "        data: List[Dict],\n",
    "        metric: str,\n",
    "        mode: str,\n",
    "        eval_func: Callable,\n",
    "        log_file_name: Optional[str] = None,\n",
    "        inference_budget: Optional[float] = None,\n",
    "        optimization_budget: Optional[float] = None,\n",
    "        num_samples: Optional[int] = 1,\n",
    "        logging_level: Optional[int] = logging.WARNING,\n",
    "        **config,\n",
    "    ):\n",
    "        \"\"\"Tune the parameters for the OpenAI API call.\n",
    "\n",
    "        TODO: support parallel tuning with ray or spark.\n",
    "        TODO: support agg_method as in test\n",
    "\n",
    "        Args:\n",
    "            data (list): The list of data points.\n",
    "            metric (str): The metric to optimize.\n",
    "            mode (str): The optimization mode, \"min\" or \"max.\n",
    "            eval_func (Callable): The evaluation function for responses.\n",
    "                The function should take a list of responses and a data point as input,\n",
    "                and return a dict of metrics. For example,\n",
    "\n",
    "        ```python\n",
    "        def eval_func(responses, **data):\n",
    "            solution = data[\"solution\"]\n",
    "            success_list = []\n",
    "            n = len(responses)\n",
    "            for i in range(n):\n",
    "                response = responses[i]\n",
    "                succeed = is_equiv_chain_of_thought(response, solution)\n",
    "                success_list.append(succeed)\n",
    "            return {\n",
    "                \"expected_success\": 1 - pow(1 - sum(success_list) / n, n),\n",
    "                \"success\": any(s for s in success_list),\n",
    "            }\n",
    "        ```\n",
    "\n",
    "            log_file_name (str, optional): The log file.\n",
    "            inference_budget (float, optional): The inference budget, dollar per instance.\n",
    "            optimization_budget (float, optional): The optimization budget, dollar in total.\n",
    "            num_samples (int, optional): The number of samples to evaluate.\n",
    "                -1 means no hard restriction in the number of trials\n",
    "                and the actual number is decided by optimization_budget. Defaults to 1.\n",
    "            logging_level (optional): logging level. Defaults to logging.WARNING.\n",
    "            **config (dict): The search space to update over the default search.\n",
    "                For prompt, please provide a string/Callable or a list of strings/Callables.\n",
    "                    - If prompt is provided for chat models, it will be converted to messages under role \"user\".\n",
    "                    - Do not provide both prompt and messages for chat models, but provide either of them.\n",
    "                    - A string template will be used to generate a prompt for each data instance\n",
    "                      using `prompt.format(**data)`.\n",
    "                    - A callable template will be used to generate a prompt for each data instance\n",
    "                      using `prompt(data)`.\n",
    "                For stop, please provide a string, a list of strings, or a list of lists of strings.\n",
    "                For messages (chat models only), please provide a list of messages (for a single chat prefix)\n",
    "                or a list of lists of messages (for multiple choices of chat prefix to choose from).\n",
    "                Each message should be a dict with keys \"role\" and \"content\". The value of \"content\" can be a string/Callable template.\n",
    "\n",
    "        Returns:\n",
    "            dict: The optimized hyperparameter setting.\n",
    "            tune.ExperimentAnalysis: The tuning results.\n",
    "        \"\"\"\n",
    "        if ERROR:\n",
    "            raise ERROR\n",
    "        space = cls.default_search_space.copy()\n",
    "        if config is not None:\n",
    "            space.update(config)\n",
    "            if \"messages\" in space:\n",
    "                space.pop(\"prompt\", None)\n",
    "            temperature = space.pop(\"temperature\", None)\n",
    "            top_p = space.pop(\"top_p\", None)\n",
    "            if temperature is not None and top_p is None:\n",
    "                space[\"temperature_or_top_p\"] = {\"temperature\": temperature}\n",
    "            elif temperature is None and top_p is not None:\n",
    "                space[\"temperature_or_top_p\"] = {\"top_p\": top_p}\n",
    "            elif temperature is not None and top_p is not None:\n",
    "                space.pop(\"temperature_or_top_p\")\n",
    "                space[\"temperature\"] = temperature\n",
    "                space[\"top_p\"] = top_p\n",
    "                logger.warning(\"temperature and top_p are not recommended to vary together.\")\n",
    "        cls._max_valid_n_per_max_tokens, cls._min_invalid_n_per_max_tokens = {}, {}\n",
    "        cls.optimization_budget = optimization_budget\n",
    "        cls.inference_budget = inference_budget\n",
    "        cls._prune_hp = \"best_of\" if space.get(\"best_of\", 1) != 1 else \"n\"\n",
    "        cls._prompts = space.get(\"prompt\")\n",
    "        if cls._prompts is None:\n",
    "            cls._messages = space.get(\"messages\")\n",
    "            if not all((isinstance(cls._messages, list), isinstance(cls._messages[0], (dict, list)))):\n",
    "                error_msg = \"messages must be a list of dicts or a list of lists.\"\n",
    "                logger.error(error_msg)\n",
    "                raise AssertionError(error_msg)\n",
    "            if isinstance(cls._messages[0], dict):\n",
    "                cls._messages = [cls._messages]\n",
    "            space[\"messages\"] = tune.choice(list(range(len(cls._messages))))\n",
    "        else:\n",
    "            if space.get(\"messages\") is not None:\n",
    "                error_msg = \"messages and prompt cannot be provided at the same time.\"\n",
    "                logger.error(error_msg)\n",
    "                raise AssertionError(error_msg)\n",
    "            if not isinstance(cls._prompts, (str, list)):\n",
    "                error_msg = \"prompt must be a string or a list of strings.\"\n",
    "                logger.error(error_msg)\n",
    "                raise AssertionError(error_msg)\n",
    "            if isinstance(cls._prompts, str):\n",
    "                cls._prompts = [cls._prompts]\n",
    "            space[\"prompt\"] = tune.choice(list(range(len(cls._prompts))))\n",
    "        cls._stops = space.get(\"stop\")\n",
    "        if cls._stops:\n",
    "            if not isinstance(cls._stops, (str, list)):\n",
    "                error_msg = \"stop must be a string, a list of strings, or a list of lists of strings.\"\n",
    "                logger.error(error_msg)\n",
    "                raise AssertionError(error_msg)\n",
    "            if not (isinstance(cls._stops, list) and isinstance(cls._stops[0], list)):\n",
    "                cls._stops = [cls._stops]\n",
    "            space[\"stop\"] = tune.choice(list(range(len(cls._stops))))\n",
    "        cls._config_list = space.get(\"config_list\")\n",
    "        if cls._config_list is not None:\n",
    "            is_const = is_constant(cls._config_list)\n",
    "            if is_const:\n",
    "                space.pop(\"config_list\")\n",
    "        cls._metric, cls._mode = metric, mode\n",
    "        cls._total_cost = 0  # total optimization cost\n",
    "        cls._eval_func = eval_func\n",
    "        cls.data = data\n",
    "        cls.avg_input_tokens = None\n",
    "\n",
    "        space_model = space[\"model\"]\n",
    "        if not isinstance(space_model, str) and len(space_model) > 1:\n",
    "            # make a hierarchical search space\n",
    "            subspace = {}\n",
    "            if \"max_tokens\" in space:\n",
    "                subspace[\"max_tokens\"] = space.pop(\"max_tokens\")\n",
    "            if \"temperature_or_top_p\" in space:\n",
    "                subspace[\"temperature_or_top_p\"] = space.pop(\"temperature_or_top_p\")\n",
    "            if \"best_of\" in space:\n",
    "                subspace[\"best_of\"] = space.pop(\"best_of\")\n",
    "            if \"n\" in space:\n",
    "                subspace[\"n\"] = space.pop(\"n\")\n",
    "            choices = []\n",
    "            for model in space[\"model\"]:\n",
    "                choices.append({\"model\": model, **subspace})\n",
    "            space[\"subspace\"] = tune.choice(choices)\n",
    "            space.pop(\"model\")\n",
    "            # start all the models with the same hp config\n",
    "            search_alg = BlendSearch(\n",
    "                cost_attr=\"cost\",\n",
    "                cost_budget=optimization_budget,\n",
    "                metric=metric,\n",
    "                mode=mode,\n",
    "                space=space,\n",
    "            )\n",
    "            config0 = search_alg.suggest(\"t0\")\n",
    "            points_to_evaluate = [config0]\n",
    "            for model in space_model:\n",
    "                if model != config0[\"subspace\"][\"model\"]:\n",
    "                    point = config0.copy()\n",
    "                    point[\"subspace\"] = point[\"subspace\"].copy()\n",
    "                    point[\"subspace\"][\"model\"] = model\n",
    "                    points_to_evaluate.append(point)\n",
    "            search_alg = BlendSearch(\n",
    "                cost_attr=\"cost\",\n",
    "                cost_budget=optimization_budget,\n",
    "                metric=metric,\n",
    "                mode=mode,\n",
    "                space=space,\n",
    "                points_to_evaluate=points_to_evaluate,\n",
    "            )\n",
    "        else:\n",
    "            search_alg = BlendSearch(\n",
    "                cost_attr=\"cost\",\n",
    "                cost_budget=optimization_budget,\n",
    "                metric=metric,\n",
    "                mode=mode,\n",
    "                space=space,\n",
    "            )\n",
    "        old_level = logger.getEffectiveLevel()\n",
    "        logger.setLevel(logging_level)\n",
    "        with diskcache.Cache(cls.cache_path) as cls._cache:\n",
    "            analysis = tune.run(\n",
    "                cls._eval,\n",
    "                search_alg=search_alg,\n",
    "                num_samples=num_samples,\n",
    "                log_file_name=log_file_name,\n",
    "                verbose=3,\n",
    "            )\n",
    "        config = analysis.best_config\n",
    "        params = cls._get_params_for_create(config)\n",
    "        if cls._config_list is not None and is_const:\n",
    "            params.pop(\"config_list\")\n",
    "        logger.setLevel(old_level)\n",
    "        return params, analysis\n",
    "\n",
    "    @classmethod\n",
    "    def create(\n",
    "        cls,\n",
    "        context: Optional[Dict] = None,\n",
    "        use_cache: Optional[bool] = True,\n",
    "        config_list: Optional[List[Dict]] = None,\n",
    "        filter_func: Optional[Callable[[Dict, Dict, Dict], bool]] = None,\n",
    "        raise_on_ratelimit_or_timeout: Optional[bool] = True,\n",
    "        allow_format_str_template: Optional[bool] = False,\n",
    "        **config,\n",
    "    ):\n",
    "        \"\"\"Make a completion for a given context.\n",
    "\n",
    "        Args:\n",
    "            context (Dict, Optional): The context to instantiate the prompt.\n",
    "                It needs to contain keys that are used by the prompt template or the filter function.\n",
    "                E.g., `prompt=\"Complete the following sentence: {prefix}, context={\"prefix\": \"Today I feel\"}`.\n",
    "                The actual prompt will be:\n",
    "                \"Complete the following sentence: Today I feel\".\n",
    "                More examples can be found at [templating](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#templating).\n",
    "            use_cache (bool, Optional): Whether to use cached responses.\n",
    "            config_list (List, Optional): List of configurations for the completion to try.\n",
    "                The first one that does not raise an error will be used.\n",
    "                Only the differences from the default config need to be provided.\n",
    "                E.g.,\n",
    "\n",
    "        ```python\n",
    "        response = oai.Completion.create(\n",
    "            config_list=[\n",
    "                {\n",
    "                    \"model\": \"gpt-4\",\n",
    "                    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "                    \"api_type\": \"azure\",\n",
    "                    \"api_base\": os.environ.get(\"AZURE_OPENAI_API_BASE\"),\n",
    "                    \"api_version\": \"2023-03-15-preview\",\n",
    "                },\n",
    "                {\n",
    "                    \"model\": \"gpt-3.5-turbo\",\n",
    "                    \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n",
    "                    \"api_type\": \"open_ai\",\n",
    "                    \"api_base\": \"https://api.openai.com/v1\",\n",
    "                },\n",
    "                {\n",
    "                    \"model\": \"llama-7B\",\n",
    "                    \"api_base\": \"http://127.0.0.1:8080\",\n",
    "                    \"api_type\": \"open_ai\",\n",
    "                }\n",
    "            ],\n",
    "            prompt=\"Hi\",\n",
    "        )\n",
    "        ```\n",
    "\n",
    "            filter_func (Callable, Optional): A function that takes in the context, the config and the response and returns a boolean to indicate whether the response is valid. E.g.,\n",
    "\n",
    "        ```python\n",
    "        def yes_or_no_filter(context, config, response):\n",
    "            return context.get(\"yes_or_no_choice\", False) is False or any(\n",
    "                text in [\"Yes.\", \"No.\"] for text in oai.Completion.extract_text(response)\n",
    "            )\n",
    "        ```\n",
    "\n",
    "            raise_on_ratelimit_or_timeout (bool, Optional): Whether to raise RateLimitError or Timeout when all configs fail.\n",
    "                When set to False, -1 will be returned when all configs fail.\n",
    "            allow_format_str_template (bool, Optional): Whether to allow format string template in the config.\n",
    "            **config: Configuration for the openai API call. This is used as parameters for calling openai API.\n",
    "                The \"prompt\" or \"messages\" parameter can contain a template (str or Callable) which will be instantiated with the context.\n",
    "                Besides the parameters for the openai API call, it can also contain:\n",
    "                - `max_retry_period` (int): the total time (in seconds) allowed for retrying failed requests.\n",
    "                - `retry_wait_time` (int): the time interval to wait (in seconds) before retrying a failed request.\n",
    "                - `seed` (int) for the cache. This is useful when implementing \"controlled randomness\" for the completion.\n",
    "\n",
    "        Returns:\n",
    "            Responses from OpenAI API, with additional fields.\n",
    "                - `cost`: the total cost.\n",
    "            When `config_list` is provided, the response will contain a few more fields:\n",
    "                - `config_id`: the index of the config in the config_list that is used to generate the response.\n",
    "                - `pass_filter`: whether the response passes the filter function. None if no filter is provided.\n",
    "        \"\"\"\n",
    "        if ERROR:\n",
    "            raise ERROR\n",
    "        if config_list:\n",
    "            last = len(config_list) - 1\n",
    "            cost = 0\n",
    "            for i, each_config in enumerate(config_list):\n",
    "                base_config = config.copy()\n",
    "                base_config[\"allow_format_str_template\"] = allow_format_str_template\n",
    "                base_config.update(each_config)\n",
    "                if i < last and filter_func is None and \"max_retry_period\" not in base_config:\n",
    "                    # max_retry_period = 0 to avoid retrying when no filter is given\n",
    "                    base_config[\"max_retry_period\"] = 0\n",
    "                try:\n",
    "                    response = cls.create(\n",
    "                        context,\n",
    "                        use_cache,\n",
    "                        raise_on_ratelimit_or_timeout=i < last or raise_on_ratelimit_or_timeout,\n",
    "                        **base_config,\n",
    "                    )\n",
    "                    if response == -1:\n",
    "                        return response\n",
    "                    pass_filter = filter_func is None or filter_func(\n",
    "                        context=context, base_config=config, response=response\n",
    "                    )\n",
    "                    if pass_filter or i == last:\n",
    "                        response[\"cost\"] = cost + response[\"cost\"]\n",
    "                        response[\"config_id\"] = i\n",
    "                        response[\"pass_filter\"] = pass_filter\n",
    "                        return response\n",
    "                    cost += response[\"cost\"]\n",
    "                except (AuthenticationError, RateLimitError, Timeout, InvalidRequestError):\n",
    "                    logger.debug(f\"failed with config {i}\", exc_info=1)\n",
    "                    if i == last:\n",
    "                        raise\n",
    "        params = cls._construct_params(context, config, allow_format_str_template=allow_format_str_template)\n",
    "        if not use_cache:\n",
    "            return cls._get_response(\n",
    "                params, raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout, use_cache=False\n",
    "            )\n",
    "        seed = cls.seed\n",
    "        if \"seed\" in params:\n",
    "            cls.set_cache(params.pop(\"seed\"))\n",
    "        with diskcache.Cache(cls.cache_path) as cls._cache:\n",
    "            cls.set_cache(seed)\n",
    "            return cls._get_response(params, raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout)\n",
    "\n",
    "    @classmethod\n",
    "    def instantiate(\n",
    "        cls,\n",
    "        template: Union[str, None],\n",
    "        context: Optional[Dict] = None,\n",
    "        allow_format_str_template: Optional[bool] = False,\n",
    "    ):\n",
    "        if not context or template is None:\n",
    "            return template\n",
    "        if isinstance(template, str):\n",
    "            return template.format(**context) if allow_format_str_template else template\n",
    "        return template(context)\n",
    "\n",
    "    @classmethod\n",
    "    def _construct_params(cls, context, config, prompt=None, messages=None, allow_format_str_template=False):\n",
    "        params = config.copy()\n",
    "        model = config[\"model\"]\n",
    "        prompt = config.get(\"prompt\") if prompt is None else prompt\n",
    "        messages = config.get(\"messages\") if messages is None else messages\n",
    "        # either \"prompt\" should be in config (for being compatible with non-chat models)\n",
    "        # or \"messages\" should be in config (for tuning chat models only)\n",
    "        if prompt is None and (model in cls.chat_models or issubclass(cls, ChatCompletion)):\n",
    "            if messages is None:\n",
    "                raise ValueError(\"Either prompt or messages should be in config for chat models.\")\n",
    "        if prompt is None:\n",
    "            params[\"messages\"] = (\n",
    "                [\n",
    "                    {\n",
    "                        **m,\n",
    "                        \"content\": cls.instantiate(m[\"content\"], context, allow_format_str_template),\n",
    "                    }\n",
    "                    if m.get(\"content\")\n",
    "                    else m\n",
    "                    for m in messages\n",
    "                ]\n",
    "                if context\n",
    "                else messages\n",
    "            )\n",
    "        elif model in cls.chat_models or issubclass(cls, ChatCompletion):\n",
    "            # convert prompt to messages\n",
    "            params[\"messages\"] = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": cls.instantiate(prompt, context, allow_format_str_template),\n",
    "                },\n",
    "            ]\n",
    "            params.pop(\"prompt\", None)\n",
    "        else:\n",
    "            params[\"prompt\"] = cls.instantiate(prompt, context, allow_format_str_template)\n",
    "        return params\n",
    "\n",
    "    @classmethod\n",
    "    def test(\n",
    "        cls,\n",
    "        data,\n",
    "        eval_func=None,\n",
    "        use_cache=True,\n",
    "        agg_method=\"avg\",\n",
    "        return_responses_and_per_instance_result=False,\n",
    "        logging_level=logging.WARNING,\n",
    "        **config,\n",
    "    ):\n",
    "        \"\"\"Evaluate the responses created with the config for the OpenAI API call.\n",
    "\n",
    "        Args:\n",
    "            data (list): The list of test data points.\n",
    "            eval_func (Callable): The evaluation function for responses per data instance.\n",
    "                The function should take a list of responses and a data point as input,\n",
    "                and return a dict of metrics. You need to either provide a valid callable\n",
    "                eval_func; or do not provide one (set None) but call the test function after\n",
    "                calling the tune function in which a eval_func is provided.\n",
    "                In the latter case we will use the eval_func provided via tune function.\n",
    "                Defaults to None.\n",
    "\n",
    "        ```python\n",
    "        def eval_func(responses, **data):\n",
    "            solution = data[\"solution\"]\n",
    "            success_list = []\n",
    "            n = len(responses)\n",
    "            for i in range(n):\n",
    "                response = responses[i]\n",
    "                succeed = is_equiv_chain_of_thought(response, solution)\n",
    "                success_list.append(succeed)\n",
    "            return {\n",
    "                \"expected_success\": 1 - pow(1 - sum(success_list) / n, n),\n",
    "                \"success\": any(s for s in success_list),\n",
    "            }\n",
    "        ```\n",
    "            use_cache (bool, Optional): Whether to use cached responses. Defaults to True.\n",
    "            agg_method (str, Callable or a dict of Callable): Result aggregation method (across\n",
    "                multiple instances) for each of the metrics. Defaults to 'avg'.\n",
    "                An example agg_method in str:\n",
    "\n",
    "        ```python\n",
    "        agg_method = 'median'\n",
    "        ```\n",
    "                An example agg_method in a Callable:\n",
    "\n",
    "        ```python\n",
    "        agg_method = np.median\n",
    "        ```\n",
    "\n",
    "                An example agg_method in a dict of Callable:\n",
    "\n",
    "        ```python\n",
    "        agg_method={'median_success': np.median, 'avg_success': np.mean}\n",
    "        ```\n",
    "\n",
    "            return_responses_and_per_instance_result (bool): Whether to also return responses\n",
    "                and per instance results in addition to the aggregated results.\n",
    "            logging_level (optional): logging level. Defaults to logging.WARNING.\n",
    "            **config (dict): parametes passed to the openai api call `create()`.\n",
    "\n",
    "        Returns:\n",
    "            None when no valid eval_func is provided in either test or tune;\n",
    "            Otherwise, a dict of aggregated results, responses and per instance results if `return_responses_and_per_instance_result` is True;\n",
    "            Otherwise, a dict of aggregated results (responses and per instance results are not returned).\n",
    "        \"\"\"\n",
    "        result_agg, responses_list, result_list = {}, [], []\n",
    "        metric_keys = None\n",
    "        cost = 0\n",
    "        old_level = logger.getEffectiveLevel()\n",
    "        logger.setLevel(logging_level)\n",
    "        for i, data_i in enumerate(data):\n",
    "            logger.info(f\"evaluating data instance {i}\")\n",
    "            response = cls.create(data_i, use_cache, **config)\n",
    "            cost += response[\"cost\"]\n",
    "            # evaluate the quality of the responses\n",
    "            responses = cls.extract_text_or_function_call(response)\n",
    "            if eval_func is not None:\n",
    "                metrics = eval_func(responses, **data_i)\n",
    "            elif hasattr(cls, \"_eval_func\"):\n",
    "                metrics = cls._eval_func(responses, **data_i)\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"Please either provide a valid eval_func or do the test after the tune function is called.\"\n",
    "                )\n",
    "                return\n",
    "            if not metric_keys:\n",
    "                metric_keys = []\n",
    "                for k in metrics.keys():\n",
    "                    try:\n",
    "                        _ = float(metrics[k])\n",
    "                        metric_keys.append(k)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            result_list.append(metrics)\n",
    "            if return_responses_and_per_instance_result:\n",
    "                responses_list.append(responses)\n",
    "        if isinstance(agg_method, str):\n",
    "            if agg_method in [\"avg\", \"average\"]:\n",
    "                for key in metric_keys:\n",
    "                    result_agg[key] = np.mean([r[key] for r in result_list])\n",
    "            elif agg_method == \"median\":\n",
    "                for key in metric_keys:\n",
    "                    result_agg[key] = np.median([r[key] for r in result_list])\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    f\"Aggregation method {agg_method} not supported. Please write your own aggregation method as a callable(s).\"\n",
    "                )\n",
    "        elif callable(agg_method):\n",
    "            for key in metric_keys:\n",
    "                result_agg[key] = agg_method([r[key] for r in result_list])\n",
    "        elif isinstance(agg_method, dict):\n",
    "            for key in metric_keys:\n",
    "                metric_agg_method = agg_method[key]\n",
    "                if not callable(metric_agg_method):\n",
    "                    error_msg = \"please provide a callable for each metric\"\n",
    "                    logger.error(error_msg)\n",
    "                    raise AssertionError(error_msg)\n",
    "                result_agg[key] = metric_agg_method([r[key] for r in result_list])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"agg_method needs to be a string ('avg' or 'median'),\\\n",
    "                or a callable, or a dictionary of callable.\"\n",
    "            )\n",
    "        logger.setLevel(old_level)\n",
    "        # should we also return the result_list and responses_list or not?\n",
    "        if \"cost\" not in result_agg:\n",
    "            result_agg[\"cost\"] = cost\n",
    "        if \"inference_cost\" not in result_agg:\n",
    "            result_agg[\"inference_cost\"] = cost / len(data)\n",
    "        if return_responses_and_per_instance_result:\n",
    "            return result_agg, result_list, responses_list\n",
    "        else:\n",
    "            return result_agg\n",
    "\n",
    "    @classmethod\n",
    "    def cost(cls, response: dict):\n",
    "        \"\"\"Compute the cost of an API call.\n",
    "\n",
    "        Args:\n",
    "            response (dict): The response from OpenAI API.\n",
    "\n",
    "        Returns:\n",
    "            The cost in USD. 0 if the model is not supported.\n",
    "        \"\"\"\n",
    "        model = response[\"model\"]\n",
    "        if model not in cls.price1K:\n",
    "            return 0\n",
    "            # raise ValueError(f\"Unknown model: {model}\")\n",
    "        usage = response[\"usage\"]\n",
    "        n_input_tokens = usage[\"prompt_tokens\"]\n",
    "        n_output_tokens = usage.get(\"completion_tokens\", 0)\n",
    "        price1K = cls.price1K[model]\n",
    "        if isinstance(price1K, tuple):\n",
    "            return (price1K[0] * n_input_tokens + price1K[1] * n_output_tokens) / 1000\n",
    "        return price1K * (n_input_tokens + n_output_tokens) / 1000\n",
    "\n",
    "    @classmethod\n",
    "    def extract_text(cls, response: dict) -> List[str]:\n",
    "        \"\"\"Extract the text from a completion or chat response.\n",
    "\n",
    "        Args:\n",
    "            response (dict): The response from OpenAI API.\n",
    "\n",
    "        Returns:\n",
    "            A list of text in the responses.\n",
    "        \"\"\"\n",
    "        choices = response[\"choices\"]\n",
    "        if \"text\" in choices[0]:\n",
    "            return [choice[\"text\"] for choice in choices]\n",
    "        return [choice[\"message\"].get(\"content\", \"\") for choice in choices]\n",
    "\n",
    "    @classmethod\n",
    "    def extract_text_or_function_call(cls, response: dict) -> List[str]:\n",
    "        \"\"\"Extract the text or function calls from a completion or chat response.\n",
    "\n",
    "        Args:\n",
    "            response (dict): The response from OpenAI API.\n",
    "\n",
    "        Returns:\n",
    "            A list of text or function calls in the responses.\n",
    "        \"\"\"\n",
    "        choices = response[\"choices\"]\n",
    "        if \"text\" in choices[0]:\n",
    "            return [choice[\"text\"] for choice in choices]\n",
    "        return [\n",
    "            choice[\"message\"] if \"function_call\" in choice[\"message\"] else choice[\"message\"].get(\"content\", \"\")\n",
    "            for choice in choices\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    @property\n",
    "    def logged_history(cls) -> Dict:\n",
    "        \"\"\"Return the book keeping dictionary.\"\"\"\n",
    "        return cls._history_dict\n",
    "\n",
    "    @classmethod\n",
    "    def start_logging(\n",
    "        cls, history_dict: Optional[Dict] = None, compact: Optional[bool] = True, reset_counter: Optional[bool] = True\n",
    "    ):\n",
    "        \"\"\"Start book keeping.\n",
    "\n",
    "        Args:\n",
    "            history_dict (Dict): A dictionary for book keeping.\n",
    "                If no provided, a new one will be created.\n",
    "            compact (bool): Whether to keep the history dictionary compact.\n",
    "                Compact history contains one key per conversation, and the value is a dictionary\n",
    "                like:\n",
    "        ```python\n",
    "        {\n",
    "            \"create_at\": [0, 1],\n",
    "            \"cost\": [0.1, 0.2],\n",
    "        }\n",
    "        ```\n",
    "                where \"created_at\" is the index of API calls indicating the order of all the calls,\n",
    "                and \"cost\" is the cost of each call. This example shows that the conversation is based\n",
    "                on two API calls. The compact format is useful for condensing the history of a conversation.\n",
    "                If compact is False, the history dictionary will contain all the API calls: the key\n",
    "                is the index of the API call, and the value is a dictionary like:\n",
    "        ```python\n",
    "        {\n",
    "            \"request\": request_dict,\n",
    "            \"response\": response_dict,\n",
    "        }\n",
    "        ```\n",
    "                where request_dict is the request sent to OpenAI API, and response_dict is the response.\n",
    "                For a conversation containing two API calls, the non-compact history dictionary will be like:\n",
    "        ```python\n",
    "        {\n",
    "            0: {\n",
    "                \"request\": request_dict_0,\n",
    "                \"response\": response_dict_0,\n",
    "            },\n",
    "            1: {\n",
    "                \"request\": request_dict_1,\n",
    "                \"response\": response_dict_1,\n",
    "            },\n",
    "        ```\n",
    "                The first request's messages plus the response is equal to the second request's messages.\n",
    "                For a conversation with many turns, the non-compact history dictionary has a quadratic size\n",
    "                while the compact history dict has a linear size.\n",
    "            reset_counter (bool): whether to reset the counter of the number of API calls.\n",
    "        \"\"\"\n",
    "        cls._history_dict = {} if history_dict is None else history_dict\n",
    "        cls._history_compact = compact\n",
    "        cls._count_create = 0 if reset_counter or cls._count_create is None else cls._count_create\n",
    "\n",
    "    @classmethod\n",
    "    def stop_logging(cls):\n",
    "        \"\"\"End book keeping.\"\"\"\n",
    "        cls._history_dict = cls._count_create = None\n",
    "\n",
    "\n",
    "class ChatCompletion(Completion):\n",
    "    \"\"\"A class for OpenAI API ChatCompletion. Share the same API as Completion.\"\"\"\n",
    "\n",
    "    default_search_space = Completion.default_search_space.copy()\n",
    "    default_search_space[\"model\"] = tune.choice([\"gpt-3.5-turbo\", \"gpt-4\"])\n",
    "    openai_completion_class = not ERROR and openai.ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mautogen\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39mgetsource(autogen\u001b[39m.\u001b[39mCompletion\u001b[39m.\u001b[39m_get_response) \u001b[39m==\u001b[39m inspect\u001b[39m.\u001b[39mgetsource(Completion\u001b[39m.\u001b[39m_get_response):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mthose 2 functions are same\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yian.xie/Desktop/autogen_bitbucket/AutoGen/test-1.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Completion' is not defined"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "if inspect.getsource(autogen.Completion._get_response) == inspect.getsource(Completion._get_response):\n",
    "  print(\"those 2 functions are same\")\n",
    "else:\n",
    "  print(\"different\")\n",
    "  \n",
    "# autogen.Completion._get_response = Completion._get_response\n",
    "# print(inspect.getsource(autogen.Completion._get_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to newAssistant):\n",
      "\n",
      "Plot a chart of NVDA and TESLA stock price change YTD.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.completion: 10-20 11:44:05] {234} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py\", line 220, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 20 Oct 2023 01:14:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '818d66f2afc01f5e-MEL', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 10-20 11:44:20] {234} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py\", line 220, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Internal server error {\n",
      "    \"error\": {\n",
      "        \"message\": \"Internal server error\",\n",
      "        \"type\": \"auth_subrequest_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"internal_error\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Internal server error', 'type': 'auth_subrequest_error', 'param': None, 'code': 'internal_error'}} {'Date': 'Fri, 20 Oct 2023 01:14:22 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '166', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': '395b4489ec02db6904d4728237a9f0a6', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '818d6732ea0f1f5e-MEL', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 10-20 11:44:30] {234} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py\", line 220, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Fri, 20 Oct 2023 01:14:32 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '818d6792a9c81f5e-MEL', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "[autogen.oai.completion: 10-20 11:44:45] {234} INFO - retrying in 10 seconds...\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/autogen/oai/completion.py\", line 220, in _get_response\n",
      "    response = openai_completion.create(request_timeout=request_timeout, **config)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 155, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 299, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 710, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/Users/yian.xie/.pyenv/versions/3.10.6/lib/python3.10/site-packages/openai/api_requestor.py\", line 775, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.APIError: Internal server error {\n",
      "    \"error\": {\n",
      "        \"message\": \"Internal server error\",\n",
      "        \"type\": \"auth_subrequest_error\",\n",
      "        \"param\": null,\n",
      "        \"code\": \"internal_error\"\n",
      "    }\n",
      "}\n",
      " 500 {'error': {'message': 'Internal server error', 'type': 'auth_subrequest_error', 'param': None, 'code': 'internal_error'}} {'Date': 'Fri, 20 Oct 2023 01:14:48 GMT', 'Content-Type': 'application/json; charset=utf-8', 'Content-Length': '166', 'Connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': 'e6493c17e619652c98a38f42feb53fbd', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '818d67d2ad1d1f5e-MEL', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    }
   ],
   "source": [
    "from autogen import AssistantAgent , UserProxyAgent, config_list_from_json\n",
    "\n",
    "\n",
    "\n",
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "assistant = AssistantAgent(\"newAssistant\", llm_config={\"config_list\": config_list})\n",
    "# assistant = newAssistantAgent(\"newAssistant\", llm_config={\"config_list\": config_list})\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\"})\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
